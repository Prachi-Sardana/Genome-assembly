# Genome-assembly
## Built a pipeline to perform a preliminary genome assembly of the source organism Listeria monocytogenes 

### 1.	Genome assembly 
To retrieve the sequence reads from NGS of the source organism, used fasterq-dump and created a shell script getNGS.sh .After the data extraction, the reads were quality trimmed using trimmomatic-0.39-2 version which is a Java based quality trimmer that uses sliding window to direct the quality scores below a specific threshold and it also removes any extra adapters that get attached to beginning or end of cDNA fragments.
Created a shell script trim.sh. In the script, specified trimmomatic paired end reads, threads as one which indicate server threads, phred33 indicating the quality encoding method. Thereafter, paired and unpaired output files are directed to get the same number of reads in the left and right file and low quality reads are eliminated. To remove the number of bases from the beginning used headcrop. Illuminaclip specifies the mismatch that are allowed in adapter match.The leading and trailing determine the minimum quality for trimming at the start and end of reads.Sliding window checks for the minimum average quality for bases in that window and the minimum length of reads to be kept.
After read trimming, the data were passed into the first stage of pipeline of genome assembly which is the SPAdes genome assembler v3.13.1. The Spades assemble the genome on the basis of overlapping reads to get the quality trimmed reads.
To run the assembly created a shell script runSpades.sh and assembler will assemble the contigs, scaffolds and create an assembly graph in GFA format. After the assembly, runned a shell script runQuast.sh which checks the quality of assembly by computing various metrices N50,L50,NG50,L50,genome fraction. Finally run sbatch_assembleGenome.sh to run the whole program and assemble the genome which will create the several output ,log files and plots.The assembly is further analysed on the basis of result.txt and reports.

### 3.	Assembling a transcriptome from RNA raw reads:-

Before taking an input RNA rawreads, built a genome database using GSNAP which use ListeriaGmap database to perform the alignment of RNA seq reads. Built a script ListeriBuild.sh for the alignment. Used scaffolds.fasta to build the database. 
Following which processed input data RNA raw reads of source organism by retrieving it from NCBI SRA using fasterq-dump command. Thereafter, used trimmomatic tool to trim the low quality reads, clean up the reads and removing adapter remnants. This generated paired and unpaired reads. After successful trimming, the reads were assembled using Trinity as it reconstructs the full length transcriptome and is better than other denovo assemblers. Created a shell script trinityDeNovo1.sh to list all the left and right reads and run them in a comma separated lists. After which , shell script runtrinityDeNovo.sh was run on CPU mode 4 and the assembly commenced with working on reconstructing the transcripts using three independent software modules Inchworm, Chrysalis and Butterfly sequentially. The jellyfish extract and count the kmers from the reads, inchworm assembles initial contigs by greedily extending sequences with most abundant kmers, chrysalis clusters the overlapping contig ,build de Bruijn graph for each cluster and partitions reads between the clusters, Butterfly resolves alternatively spliced and paralogous transcripts independently for each cluster.This will result in output files in trinity denovo that will be generated in results. To analyse the trinity results created a shell script analyzetrinityDenovo.sh which stored the standard output to results as text file counting the number of trinity genes,transcripts and generate a statistics of all the transcript contigs and stats based on longest isoform per gene. Finally created a shell script sbatch_trinityDenovo.sh to combine all the shell scripts and create the desired output files and directories

### 3.	Annotation:- 
To annotate genes within the genome used blast, Transdecoder and hmmscan to identify protein sequences similarity and protein predictions using input data .Input data containing Trinity de novo data of Listeria genome from previous module was extracted and proteins were predicted using Transdecoder which follows a 4 step process:
1.	Transdecoder.long orf - which searches the longest open reading frame and translate to amino acid sequences.
2.	Blastp - which identifies proteins similarity by aligning the long orf to guide the prediction expression process
3.	Hmmscan - which uses Hidden markov model to scan and protein domains and guide the prediction process
4.	Transdecoder.predict - which takes in ORF, BLAST Output and protein domain information to filter protein predictions to produce a protein fasta file *.pep
To begin with the protein prediction , the directories, files, results and scripts were created The bash scripts were run sequentially (longOrfs_args.sh, blastPep_args.sh,pfamScan_args.sh,predictProteins_args.sh) to extend command line arguments to make the code extensible in provided path.
The scripts were hard coded using $preceded by the number of arguments.
•	longOrf_args.sh - finds the longest ORF in data taking trinity fasta as an input and producing an output in results in trinity_de_novo_transdecoder_dir
•	blastPep.sh - aligns the proteins which are found in longest orf to swissprot database
•	pfamScan_args.sh - used hmmscan tool to take query protein sequences found in the ORF's and searched them against HMM database and output the ranked list of significant matches to the sequence.
•	predictProteins_args.sh - The transdecoder predicts the coding regions from the ORFS according to the ORF retention criteria which is as follows:
1.	Minimum length of ORF in a transcript sequence
2.	A log likelihood score
3.	Longest orf identified if ORF is fully encapsulated by coordinates of another candidate orf
4.	PSSM (Position specific scoring matrix) computed to refine start codon prediction
•	Finally run the command sbatch_transdecoder.sh script to perform BLAST and Transdecoder.
5.	Predicting the function for those genes: - The aligned predicted proteins were further used as an input data to align them to Swissprot using BLAST.
Used KEGG (Kyoto Encyclopedia of genes and genomes) and gene ontology to retrieve the data for set of transcripts and combine the data into single annotation file.Associated SwissProt protein IDs with the transcripts to understand the function of the genes. To predict the function used KEGG API which is a programmatic way to access the data from a service like KEGG. In order to convert Swissprot to KEGG for protein and write the results in a tabular file, created a function getUniprotfromBlast to return the Uniprot ID from the blast line if evalue is below the threshold.If the evalue is above threshold, it returns False. To return list of all kegg genes connected with Uniprot id created a function getKeggGene. After having uniprot id , to get kegg id we need to pass it to Kegg API. To execute KEGG api command used requests library. In order to decode the data into normal text used decode() string method and encoding type (UTF-8). To iterate over more than one line of output used result.iter_lines() and returned kegg genes. Similarly created a function getKeggOrthology() to return KeggOrthologyID using kegg genes and modeled it on getKegggenes() function. Created a function load kegg pathways to return the dictionary of pathID as keys and pathway description as values. Finally created a function addkeggPathways() to tie all the functions together to establish an output file in results as alignpredicted.txt.

